{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqLTOoUzqUf1PZejTV03nf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poietes5150/SuperCoding/blob/main/PyTorch_nn_Module%2C_Loss_Function%2C_Optimizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# nn.Module이란?\n",
        "`nn.Module`은 PyTorch에서 **모델의 기본단위**입니다. 모델의 계층(layer)과 연산을 정의하고, 학습 가능한 파라미터를 포함합니다.\n",
        "사용자가 새로운 모델을 만들기 위해서는 `nn.Module`을 상속한 클래스를 정의하고, `__init__()`에서 계층을 선언하고, `forward()`에서 데이터를 어떻게 흐르게 할지 정의합니다."
      ],
      "metadata": {
        "id": "03ZM29c0I8vJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 예시 1: 간단한 선형 회귀 모델"
      ],
      "metadata": {
        "id": "hNi8qPp-Jn3A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qybIIijOI7Nm",
        "outputId": "f9c6a3c9-ab71-4dc9-ea8a-0da91dbbcf3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearRegressionModel(\n",
            "  (linear): Linear(in_features=1, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 선형 회귀 모델: 입력 1개 -> 출력 1개\n",
        "class LinearRegressionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinearRegressionModel, self).__init__()\n",
        "        self.linear = nn.Linear(1, 1) # 입력, 출력 차원\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "model = LinearRegressionModel()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 예시 2: 다층 퍼셉트론(MLP)"
      ],
      "metadata": {
        "id": "GXWlJr9TKSuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력: 784차원 (28 x 28 이미지), 출력: 10개 클래스\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MLP, self).__init__()\n",
        "    self.layers = nn.Linear(784, 128)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.output = nn.Linear(128, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.layers(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.output(x)\n",
        "    return x\n",
        "\n",
        "mlp_model = MLP()\n",
        "print(mlp_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQB2zxfbKW5N",
        "outputId": "08ab96ff-7eba-4fe4-e948-18057f8772ea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (layers): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (output): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizer란?\n",
        "Optimizer는 신경망의 가중치를 학습하기 위한 알고리즘입니다. 주어진 손실 함수(loss function)를 줄이기 위해 가중치를 어떻게 조정할지 결정합니다.\n",
        "PyTorch에서는 `torch.optim` 모듈을 통해 다양한 Oprimizer를 제공합니다. 가장 대표적인 Optimizer는 `SGD`, `Adam` 등이 있습니다."
      ],
      "metadata": {
        "id": "gVrYZdm5MctF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 예시 1: SGD와 Adam Optimizer 사용법"
      ],
      "metadata": {
        "id": "S1l9LOlaM39M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "model = LinearRegressionModel()\n",
        "\n",
        "# SGD Optimizer\n",
        "optimizer_sgd = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Adam Optimizer\n",
        "optimizer_adam = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(f\"SGD Optimizer: {optimizer_sgd}\")\n",
        "print(f\"Adam Optimizer: {optimizer_adam}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pH_pHiVMM9hh",
        "outputId": "7faafc7f-5f27-4362-b725-5aca93b4f726"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD Optimizer: SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    differentiable: False\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.01\n",
            "    maximize: False\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "Adam Optimizer: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 손실 함수(Loss Function)란?\n",
        "모델의 출려과 실제 정답(label)의 차이를 수치화한 함수입니다. 이 값을 최소화하는 방향으로 Optimizer가 학습을 진행합니다.\n",
        "PyTorch는 `torch.nn` 에서 다양한 손실 함수를 제공합니다."
      ],
      "metadata": {
        "id": "ahSramdeN0sn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 예시: MSELoss(회귀), CrossEntropyLoss(분류)"
      ],
      "metadata": {
        "id": "sg0yGgPTO-8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 회귀 문제용 손실 함수\n",
        "mse_loss = nn.MSELoss()\n",
        "\n",
        "# 분류 문제용 손실 함수\n",
        "cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# 사용예시\n",
        "output = torch.tensor([[0.2],[0.8]], requires_grad=True)\n",
        "target = torch.tensor([[0.0], [1.0]])\n",
        "print(f\"MSE Loss: {mse_loss(output, target)}\")\n",
        "\n",
        "logits = torch.tensor([[2.0, 0.5]], requires_grad=True)\n",
        "target = torch.tensor([0])\n",
        "print(f\"Cross Entropy Loss: {cross_entropy_loss(logits, target)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hARr6jIsOLlZ",
        "outputId": "5f4c3998-2484-49e0-e3e5-2b4792b65473"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE Loss: 0.03999999910593033\n",
            "Cross Entropy Loss: 0.2014133334159851\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch DataLoader\n",
        "PyTorch의 `DataLoader`를 사용하는 두가지 방식:\n",
        "1. `TensorDataset`\n",
        "2. 사용자정의 `Custom Dataset`\n",
        "\n"
      ],
      "metadata": {
        "id": "b6jDcEHaPKY9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. TensorDataset + DataLoader 예제"
      ],
      "metadata": {
        "id": "mm7bfmEtPwYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# 임의 데이터 생성\n",
        "X = torch.randn(10, 3)          # 입력: 10개 샘플, 3 features\n",
        "y = torch.randint(0, 2, (10,))  # 레이블: 0 또는 1\n",
        "\n",
        "# TensorDataset으로 묶기\n",
        "dataset = TensorDataset(X, y)\n",
        "\n",
        "# DataLoader로 배치화\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# 데이터 순회\n",
        "print(f\"x: {X}\")\n",
        "print(f\"y: {y}\")\n",
        "print(\"=\"*30)\n",
        "for batch_X, batch_y in dataloader:\n",
        "    print(f\"입력 배치: {batch_X}\")\n",
        "    print(f\"레이블 배치: {batch_y}\")\n",
        "    print(\"-\"*30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSVJ8JNHPtRF",
        "outputId": "41fbef5a-144a-466f-c9da-1c9e2384eb44"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: tensor([[-0.4148,  0.6576, -0.7713],\n",
            "        [ 0.5132, -0.2951, -0.7301],\n",
            "        [ 0.4911,  0.5706,  1.4262],\n",
            "        [-0.1623,  0.2304, -0.9744],\n",
            "        [-1.6842,  1.5444,  1.2828],\n",
            "        [ 0.5560, -1.0305,  0.4646],\n",
            "        [ 0.0793, -0.8120, -0.1013],\n",
            "        [-0.3016, -1.0444, -2.0392],\n",
            "        [ 0.0228,  0.9327, -1.1461],\n",
            "        [ 0.9835,  1.4618, -1.0320]])\n",
            "y: tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0])\n",
            "==============================\n",
            "입력 배치: tensor([[ 0.5132, -0.2951, -0.7301],\n",
            "        [-1.6842,  1.5444,  1.2828],\n",
            "        [ 0.0228,  0.9327, -1.1461],\n",
            "        [-0.1623,  0.2304, -0.9744]])\n",
            "레이블 배치: tensor([0, 1, 0, 1])\n",
            "------------------------------\n",
            "입력 배치: tensor([[ 0.5560, -1.0305,  0.4646],\n",
            "        [-0.4148,  0.6576, -0.7713],\n",
            "        [ 0.9835,  1.4618, -1.0320],\n",
            "        [-0.3016, -1.0444, -2.0392]])\n",
            "레이블 배치: tensor([1, 1, 0, 0])\n",
            "------------------------------\n",
            "입력 배치: tensor([[ 0.0793, -0.8120, -0.1013],\n",
            "        [ 0.4911,  0.5706,  1.4262]])\n",
            "레이블 배치: tensor([0, 0])\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Custom Dataset 클래스 정의\n",
        "더 복잡하거나 전처리가 필요한 데이터일 경우 `torch.utils.data.Dataset`을 상속하여 사용자 정의 클래스를 만듭니다."
      ],
      "metadata": {
        "id": "ihB8tfVuQpLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "# 예제: x는 [0, 1, ...1 99], y는 x * 2\n",
        "class MyCustomDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        self.x = torch.arange(100, dtype=torch.float32)\n",
        "        self.y = self.x * 2\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.y[idx]\n",
        "\n",
        "# 인스턴스 생성 및 DataLoader 적용\n",
        "custom_dataset = MyCustomDataset()\n",
        "custom_loader = DataLoader(custom_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "for x, y in custom_loader:\n",
        "  print(f\"x: {x}\")\n",
        "  print(f\"y: {y}\")\n",
        "  print(\"-\"*30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "An3OCBFWQ06v",
        "outputId": "9d8910cd-cd77-46b4-c6bc-9e30ca37b8a2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: tensor([11.,  9., 16., 65., 45., 61., 19., 90.])\n",
            "y: tensor([ 22.,  18.,  32., 130.,  90., 122.,  38., 180.])\n",
            "------------------------------\n",
            "x: tensor([21., 38., 94., 50., 70., 74., 23., 40.])\n",
            "y: tensor([ 42.,  76., 188., 100., 140., 148.,  46.,  80.])\n",
            "------------------------------\n",
            "x: tensor([22., 54., 51., 25.,  4., 34., 37., 93.])\n",
            "y: tensor([ 44., 108., 102.,  50.,   8.,  68.,  74., 186.])\n",
            "------------------------------\n",
            "x: tensor([10., 55., 77., 99., 52., 95., 97., 86.])\n",
            "y: tensor([ 20., 110., 154., 198., 104., 190., 194., 172.])\n",
            "------------------------------\n",
            "x: tensor([49., 39., 15., 62.,  7., 43.,  3., 36.])\n",
            "y: tensor([ 98.,  78.,  30., 124.,  14.,  86.,   6.,  72.])\n",
            "------------------------------\n",
            "x: tensor([ 1., 24., 27., 66., 81., 47., 53., 26.])\n",
            "y: tensor([  2.,  48.,  54., 132., 162.,  94., 106.,  52.])\n",
            "------------------------------\n",
            "x: tensor([60., 14., 76., 64., 56., 84., 28.,  6.])\n",
            "y: tensor([120.,  28., 152., 128., 112., 168.,  56.,  12.])\n",
            "------------------------------\n",
            "x: tensor([88.,  5.,  8., 57., 46., 67., 42., 20.])\n",
            "y: tensor([176.,  10.,  16., 114.,  92., 134.,  84.,  40.])\n",
            "------------------------------\n",
            "x: tensor([82., 71., 68., 12., 79., 31.,  2., 58.])\n",
            "y: tensor([164., 142., 136.,  24., 158.,  62.,   4., 116.])\n",
            "------------------------------\n",
            "x: tensor([35., 32., 69., 98., 33., 63., 17., 85.])\n",
            "y: tensor([ 70.,  64., 138., 196.,  66., 126.,  34., 170.])\n",
            "------------------------------\n",
            "x: tensor([72.,  0., 73., 29., 48., 96., 83., 44.])\n",
            "y: tensor([144.,   0., 146.,  58.,  96., 192., 166.,  88.])\n",
            "------------------------------\n",
            "x: tensor([13., 89., 80., 75., 41., 78., 91., 87.])\n",
            "y: tensor([ 26., 178., 160., 150.,  82., 156., 182., 174.])\n",
            "------------------------------\n",
            "x: tensor([59., 18., 92., 30.])\n",
            "y: tensor([118.,  36., 184.,  60.])\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DataLoader 요약\n",
        "* `TensorDataset`은 간단한 텐서 데이터를 빠르게 묶는데 유용합니다.\n",
        "* `Custom Dataset`은 파일 읽기, 전처리, 다양한 타입의 데이터를 처리할 때 강력합니다.\n",
        "\n",
        "두 방법 모두 `DataLoader`와 함께 쓰여 **배치 처리, 셔플링, 멀티스레딩**이 가능합니다."
      ],
      "metadata": {
        "id": "yZTJnz3mPvBX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 연습문제"
      ],
      "metadata": {
        "id": "DeeoNoJ4UwlM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1.** 입력이 100차원이고 은닉층이 64, 출력이 10인 MLP 모델을 정의해보세요.\n",
        "\n",
        "**Q2.** `SGD` Optimizer를 사용하여 위 모델의 파라미터를 학습하려고 합니다. Optimizer코드를 작성해 보세요.\n",
        "\n",
        "**Q3.** 다중 클래스 분류 문제에 적합한 손실 함수를 선택하고, 예시 코드를 작성해보세요."
      ],
      "metadata": {
        "id": "o4Nl6M27VS6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q1. 입력이 100차원이고 은닉층이 64, 출력이 10인 MLP 모델을 정의해보세요.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class model_MLP(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(model_MLP, self).__init__()\n",
        "    self.layers = nn.Linear(100, 64)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.output = nn.Linear(64, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.layers(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.output(x)\n",
        "    return x\n",
        "\n",
        "mlp_model = MLP()\n",
        "print(mlp_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQnecWckU36f",
        "outputId": "ccbd748b-8316-4277-e488-aa7a8933612c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (layers): Linear(in_features=100, out_features=64, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2. SGD Optimizer를 사용하여 위 모델의 파라미터를 학습하려고 합니다. Optimizer코드를 작성해 보세요.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "model = model_MLP()\n",
        "optimizer_sgd = optim.SGD(model.parameters(), lr=0.01)\n",
        "print(f\"SGD Optimizer: {optimizer_sgd}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhcA9uAzWk_4",
        "outputId": "a41db3b2-adf4-47c2-bc5f-a4da7a621746"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD Optimizer: SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    differentiable: False\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.01\n",
            "    maximize: False\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3. 다중 클래스 분류 문제에 적합한 손실 함수를 선택하고, 예시 코드를 작성해보세요.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "input = torch.tensor(torch.rand(1,100), requires_grad=True)\n",
        "target = torch.tensor(torch.randint(0,2,(1,)))\n",
        "output = model(input)\n",
        "loss = cross_entropy_loss(output, target)\n",
        "print(f\"예측값: {output}\")\n",
        "print(f\"손실값: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSILpz49XReM",
        "outputId": "f3f0525f-e073-48a1-9f8c-0f48ca94e7d9"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "예측값: tensor([[-0.0804, -0.1832,  0.0663,  0.4076, -0.1773,  0.0203, -0.2480, -0.1736,\n",
            "          0.3634, -0.1535]], grad_fn=<AddmmBackward0>)\n",
            "손실값: 2.4959259033203125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-50-dc95940f935c>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input = torch.tensor(torch.rand(1,100), requires_grad=True)\n",
            "<ipython-input-50-dc95940f935c>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  target = torch.tensor(torch.randint(0,2,(1,)))\n"
          ]
        }
      ]
    }
  ]
}