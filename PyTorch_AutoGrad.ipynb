{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4YcFB2x6goMFypoWIV+zc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poietes5150/SuperCoding/blob/main/PyTorch_AutoGrad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorch2 - PyTorch 모델 구성 요소 이해\n",
        "PyTorch에서 신경망을 구성하는 핵심 클래스들인 `Autograde`, `nn.Module`, `Optimizer`, `Loss Function`, `DataLoader`에 대해 배워 봅니다.\n",
        "\n",
        "*   Reference\n",
        "    *   https://pytorch.org/docs/stable/tensors.html\n",
        "    *   https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n",
        "    *   https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fgi-XP20Wu71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch Autograd 기능\n",
        "PyTorch의 자동 미분 시스템인 **`autograd`**를 이해하고 실습해봅니다. 기울기 계산, 연산 그래프 추적, **`.backward()`, `.grad`**속성 등을 실제 코드와 함께 설명합니다.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GkUzhzSwXqnf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autograd란?\n",
        "\n",
        "`autograd`는 PyTorch에서 **자동으로 기울기를 계산**해주는 엔진입니다. 수학적으로는 '연산 그래프(computational graph)'를 구성하고, `backward()`를 호출했을때 그래프를 따라 미분을 수행합니다.\n",
        "\n",
        "PyTorch는 모든 텐서 연산을 추적하여, 이후` .backward()`를 호출하면 출력에 대한 입력의 미분 값을 `.grad` 속성에 자동으로 저장해줍니다."
      ],
      "metadata": {
        "id": "l9OT9qfBYwvB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 예제1:스칼라 연산 미분"
      ],
      "metadata": {
        "id": "JngnU2aFY3qg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ha8HwW-bWfSY",
        "outputId": "5a16d185-47e0-4580-f03a-e8f8d55059f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x의 값: 2.0\n",
            "y의 값: 14.0\n",
            "dy/dx = 7.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# 입력 텐서 (미분 추적 설정)\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "# 연산 수행\n",
        "y = x**2 + 3 * x + 4\n",
        "\n",
        "# 역전파\n",
        "y.backward()\n",
        "\n",
        "# 결과 출력\n",
        "print(f\"x의 값: {x.item()}\")\n",
        "print(f\"y의 값: {y.item()}\")\n",
        "print(f\"dy/dx = {x.grad}\")  # 미분 결과\n",
        "# print(f\"x에 대한 y의 미분값: {x.grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "설명\n",
        "\n",
        "* `requires_grad=True`는 해당텐서가 미분 대상임을 의미합니다.\n",
        "* `y.backward()` 는 y를 기준으로 역방향으로 그래프를 따라가며 미분을 수행합니다.\n",
        "* `x.grad`에는 `dy/dx` 결과가 저장됩니다.\n",
        "* 위 함수의 도함수는:     **dy/dx = 2x + 3 => 2 x 2 + 3 = 7**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "dQHMUOzQa5Tq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 예제 2: 벡터 연산 및 **`.backward()`** 사용"
      ],
      "metadata": {
        "id": "YpoAmWmncIP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "y = x * 2\n",
        "y.backward()\n",
        "print(x.grad) # what?"
      ],
      "metadata": {
        "id": "_kGjeH6Hcepx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 설명\n",
        "* `.backward()`는 **스칼라 함수**에 대해서만 호출 가능한데 `y`는 벡터이므로 직접 `y.backward()`는 불가능합니다.\n",
        "* autograd.backward()는 기본적으로 스칼라 출력에 대한 입력의 gradient(dy/dx)를 계산하는 함수입니다.\n",
        "* 하지만 y가 벡터이면 각 요소마다 어떤 방향으로 미분할지 모호합니다.\n",
        "* PyTorch는 이럴 때, 사용자에게 어떤 weight로 각 요소를 조합해서 스칼라로 만들지 알려달라고 요구합니다.\n",
        "    * gradient vector (weights) 직접 제공하여 해결 가능\n",
        "* 또는 `sum()`이나 `mean()`dmfh wnfdudi gkqslek.\n",
        "    * z = 2x(1) + 2x(2) + 2x(3) 이므로 dz/dx = 2"
      ],
      "metadata": {
        "id": "6EClptI-crGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "y = x * 2\n",
        "\n",
        "# y가 벡터일 때 backward()는 scalar여야 하므로 sum 사용\n",
        "z = y.sum() # z = 2x1 + 2x2 + 2x3 = 12\n",
        "z.backward()\n",
        "print(x.grad) # 각각 2.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXIuYDY6d41k",
        "outputId": "4076ee5f-5cc6-42aa-dd95-0aac0a8565a5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2., 2., 2.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "y = x * 2\n",
        "\n",
        "# y가 벡터일 때 backward()는 scalar여야 하므로 sum 사용\n",
        "# 직접 gradient vector(weight) 제공\n",
        "external_grad = torch.tensor([1.0, 1.0, 1.0])\n",
        "y.backward(gradient=external_grad)\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hi9cUtQMed2n",
        "outputId": "77e35ac2-3d74-4fae-e9a6-6b7aa7a623c5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2., 2., 2.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 예제 3: z에 대한 w의 미분 값 dz/dw"
      ],
      "metadata": {
        "id": "PqXcsvz5fLXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# z에 대한 w의 미분값을 계산합니다.\n",
        "w = torch.tensor(2.0, requires_grad=True)\n",
        "y = w**2\n",
        "z = 10*y + 50\n",
        "z.backward()\n",
        "print(f\"dz/dw = {w.grad.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGoXq6sJeh-A",
        "outputId": "a79b7d59-4db6-4ca4-b803-f751c1c19e91"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dz/dw = 40.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 예제 4: Q에대한 a와 b의 gradient dQ/da, dQ/db"
      ],
      "metadata": {
        "id": "Jgfm10IvfxWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q에 대한 a의 gradient\n",
        "a = torch.tensor([2., 3.], requires_grad=True)\n",
        "b = torch.tensor([6., 4.], requires_grad=True)\n",
        "\n",
        "Q = 3*a**3 - b**2\n",
        "external_grad = torch.tensor  ([1., 1.])\n",
        "Q.backward(gradient=external_grad)\n",
        "print(a.grad)\n",
        "\n",
        "# Q에 대한 b의 gradient\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOYvKoVRgZB3",
        "outputId": "2d5ecaf6-60ab-4290-d1a9-e254178fdd75"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([36., 81.])\n",
            "tensor([-12.,  -8.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autograd 비활성화: `requires_grad=False`"
      ],
      "metadata": {
        "id": "KQ5LI8IVhUCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(3.0, requires_grad=False)\n",
        "y = x * 2\n",
        "\n",
        "print(f\"y requires_grad? {y.requires_grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUMhlHEvhZLd",
        "outputId": "3e6faca8-3bf7-4bcc-925e-85fe71c40ea1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y requires_grad? False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autograd 비활성화: `torch.no_grad()`"
      ],
      "metadata": {
        "id": "kwpIbVMthyxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "with torch.no_grad():\n",
        "  y = x * 2\n",
        "  print(f\"y requires_grad? {y.requires_grad}\") # False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MukzR6zjhvUq",
        "outputId": "802645ff-1cdc-49fc-d16a-a1a883293b73"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y requires_grad? False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "설명\n",
        "* with torch.no_grad() 블록 안에서는 autograd가 작동하지 않습니다.\n",
        "* 학습이 필요업는 예측 단계나 추론시 사용됩니다. (속도 개선)\n",
        "\n"
      ],
      "metadata": {
        "id": "p3NK7x8Rhr01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autograd 연결 끊기:`.detach()`"
      ],
      "metadata": {
        "id": "ZVRZskk3iUGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(3.0, requires_grad=False)\n",
        "y = x * 2\n",
        "z = y.detach()\n",
        "\n",
        "print(f\"z requires_grad? {z.requires_grad}\") # False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aF08leayiZTf",
        "outputId": "f60bee5b-b545-4900-842e-6a23e51e4c12"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "z requires_grad? False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "설명\n",
        "* `.detach()`는 autograd 연결이 끈긴 텐서를 반환합니다.\n",
        "* 원본 텐서와는 값은 같지만, `.backward()`에 영향을 주지 않습니다."
      ],
      "metadata": {
        "id": "HGHlEhOmiqD4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LmgAC_ORikNj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}